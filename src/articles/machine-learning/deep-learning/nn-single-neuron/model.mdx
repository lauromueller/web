---
title: 'Programming a single neuron - Part 3: Building the model'
date: '2020-01-10'
field: machine-learning
area: deep-learning
slug: nn-single-neuron-model
series: nn-single-neuron
category: no category
draft: False
tags: ['deep learning', 'neural networks']
---

# 4. The math

For us to be able to move forward and understand what is happening during the model training, we have to spend some time on its mathematical foundations. This section discusses four main topics: what exactly is happening in logistic regression, how we are using our single-neuron neural network to learn a logistic regression classifier, how the single-neuron is represented mathematically (and its several elements), and how the learning process actually takes place. Once we discuss these topics, coding the model becomes a much simpler task because we know what we are trying to achieve.

## 4.1 The basics of logistic regression

The scope of this article is not to discuss logistic regression in detail, but we need to have a basic understanding of how it works and what it tries to achieve.

Informally speaking, logistic regression is a type of regression that aims at calculating the probability of an instance of belonging to some class or representing some event. In its vanilla form, a logistic regression model computes the weighted sum of its inputs and maps this result to the $[0, 1]$ interval, which is then interpreted as the probability of the instance belonging or not to the target class. More formally, a logistic regression classifier takes an input vector of dimension $n$ and outputs a single real value $\in [0, 1]$:

$$
f: \mathbb{R}^{n} \rightarrow [0, 1]
$$

This value can then be transformed into the final binary classification by checking whether it is higher than a certain predefined threshold. For example, we may say that any value greater than $0.5$ is already considered a positive hit (otherwise it is a negative hit), or we may set the threshold at another arbitrary value. In both cases, there are two steps involved in the transformation.

### 4.1.1 Computing the weighted sum of the input features

The first step involves computing the weighted sum of the input features by multiplying each of them by a certain weight. If we denote $\vec{x}$ as our input vector and $\vec{w}$ as the vector of weights, we have the following expression:

$$
g(\vec{x}) = \sum_{i=0}^{n} w_i x_i
$$

Or, in vector representation (so that we start getting used to the notation and dimensions):

$$
g(\vec{x}) =
\begin{bmatrix}
w_1 & w_2 & \cdots & w_n
\end{bmatrix}
\begin{bmatrix}
x_1\\
x_2\\
\vdots \\
x_n
\end{bmatrix}
$$

Which has the following dimensions:

$$
(1, n) \times (n, 1) \rightarrow (1, 1)
$$

The result is a real-valued number, which is then transformed by the next step into our final probability of class membership.

### 4.1.2 Transforming the weighted sum into a $[0, 1]$-valued output

The transformation is done by applying a function to map the output of $g(\vec{x})$ to the $[0, 1]$ interval. There are several functions that are used in practice, but for our purposes here we will use the standard logistic function

$$
f(x) = \dfrac{L}{1 + e^{-k(x-x_0)}}
$$

With $L=1, x_0=0, k=1$, thus leading to the simpler formula

$$
f(x) = \dfrac{1}{1 + e^{-x}}
$$

For notation purposes, we will denote this function as $\sigma (x)$, so from now on we set on the following definition (and on calling the following function simply "sigmoid function"):

$$
\sigma (x) = \dfrac{1}{1 + e^{-x}}
$$

The sigmoid function has the following shape:

[IMAGE SIGMOID FUNCTION]

There are several properties I would like to draw your attention to:

1. The function is monotonic
2. The function is increasing
3. The function's range is $(0, 1)$

These three characteristics make our function well-suited to be used as the input for binary classification. While it is incorrect to call this a probability density function (since $\int_{-\infty}^{\infty} \sigma(x) \neq 1$), the interpretation of its output as the probability of the input vector being an instance of the positive class is fairly straightforward.

These two steps give us the necessary intuition behind how an input vector is transformed into a final probability of class membership, and we can now move to the learning and modeling stages of building our model.

## 4.2 What we are trying to learn

As we saw in the previous section, a logistic regression classifier has three main components: an input vector, a vector of weights, and a transformation function that maps the real-valued linear combination of the input features to the $[0, 1]$ space. The input vector is known, and the transformation function is normally chosen to be the sigmoid function, so the only remaining piece of the puzzle is the weight vector $\vec{w}$. This is exactly what our single-neuron classifier will learn during the training phase.

In more precise terms, our goal is to learn the weight vector so that we minimize the prediction error of our logistic regression classifier. This involves choosing a function to represent the error of our classifier. This function is normally called the loss function, and it may take on many shapes. However, for our algorithm in this article we are interested in choosing a loss function that has single local minimum, thus allowing us to optimize the weight vector more easily. Of course, it will not always be the case that we have a smooth, well-behaved loss function. In more realistic scenarios, optimization methods such as the gradient descent are likely to be stuck in local minima, and this is one reason why we should run the same algorithm several times with random weight initialization to ensure that the algorithm is robust against such variations (and we didn't just get lucky on a single run).

## 4.3 The mathematical representation of a single-layer, single-neuron neural network

### 4.3.1 From the neuron's output to the binary classification

## 4.4 Backpropagation and the learning process

# 5. Implementing logistic regression with a single neuron
